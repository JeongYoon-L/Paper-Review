Context-Aware Multi-View Summarization Network for Image-Text Matching (2020)
by Leigang Qu , Meng Liu, Da Cao, Liqiang Nie, Qi Tian

Most prior work is still confronted with a multi-view description challenge.
e.g., how to align an image to multiple textual descriptions with semantic diversity.

==> Present a novel context-enhanced visual region information from multiple views.
& Design an adaptive gating self-attention module to extract representations of visual regions and words.

##Problem of prior works
They commonly embed images and texts as global vectors into a unified semantic space, whereby cross-modal similarities can be calculated for matching
--> Use of attention mechanism to perfomr semantic alignments, particularly, they aggregate fragment features guided by local cross-modal affinities, to obtain global representations.
--> But they also ignore the effect of intra-modal context information and takes unavoidable computational cost for the pair-wise similarity estimation.
==> Suggest Context-Aware Multi-View Summarizaation Network; CAMERA for short.
(exploits the context-aware representations and aggregates visual regions based on a multi-view summarization module.

##Introduction
Firstly, apply the bottom-up attention to identify the salient regions at the object/stuff level.
Thereafter, introduce an adaptive gating self-attention (AGSA) module to exploit the intra-modal context for images and texts, which is capable of controlling the information flow more flexibly.
==> Devise a multi-view summarization module to aggregate region level features into multiple image-level embeddings from different information overlap among different views.
Also, to learn semantic alignment in the joint space, adopt a bidirectional triplet loss with hard negative sample mining.

## Related work (Nice summary!)
ITM

##Proposed Method
CAMERA : image embedding branch, text embedding branch, loss function


##3.1 AGSA (Adaptive Gating Self Attention)
3.1.1 Multi-Head Self Attention
Q, K, V (Query, Key, Value)
*Self-attention mechanism : Using Softmax for each row.
*multi-Head self attention : H paralleled self-attention mechanisms, to capture context information from different subspaces,
X : input
Wi^Q, Wi^K, Wi^V : the learnable projection matrices for query, key and value.
dk = dv= d/H

--> exploit the intra-modal interactions by calculating the dot-product similarities between queries and keys.
(may contain noisy or meaningless information)

3.1.2 Gate Mechanism
So, use fusion strategy.
Gi : fusion result
WG^Q and WG^K are the learnable projection matrices
b is bias vectors.
And then, the gating masks Mi^Q , Mi^K corresponding to Qi and Ki are generated by adopting two fully connected layers followed by signmoid function.

WM^Q, WM^K, bM^Q, bM^K are the parameters of fully connected layers
And get hi~ which is the gating self-attention result of the ith head.
And then get the result of AGSA with concatenating the results of multiple heads.

3.2 Image Embedding



##Keywords : Image-Text Matching, Cross-Modal Retrieval, Multi-View Summarization, Context Modeling

## Dataset
Flickr30K and MS-COCO


## Terminology
1. Cross-Modal : The term cross-modal learning refers to the synergistic synthesis of information from multiple sensory modalities such that the learning that occurs within any individual sensory modality can be enhanced with information from one or more other modalities. 
--> https://www.frontiersin.org/research-topics/9154/cross-modal-learning-adaptivity-prediction-and-interaction
e.g, this case, visual & Textual
2. fusion result?
3. learnable projection matrices
4. ensemble
5. attention query, key, value : https://wikidocs.net/22893
병렬 : parallel 직렬 : series
6. attention : https://welcome-to-dewy-world.tistory.com/108

